import os
import time
import argparse
import json
import random
import requests
import torch
import base64
import matplotlib.pyplot as plt
from openai import OpenAI
from diffusers import StableDiffusionPipeline, StableDiffusion3Pipeline, DiffusionPipeline, DPMSolverMultistepScheduler
import subprocess
import re
import cv2
import json
import bleach
import argparse
from tqdm import tqdm
from torch.utils.data import DataLoader, DistributedSampler
from transformers import AutoTokenizer, CLIPImageProcessor
import torch
import os
import json
import warnings
warnings.filterwarnings("ignore")

import hpsv2
from PIL import Image
import numpy as np

def hpsv2_score(image_path, prompt, hps_version="v2.1"):

    scores = hpsv2.score(image_path, prompt, hps_version=hps_version)
    return scores[0] if isinstance(scores, (list, np.ndarray)) else scores

# get gpt score
def gptv_query(transcript=None, temp=0.):
    max_tokens = 1024
    wait_time = 0

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    data = {
        # 'model': 'gpt-4-vision-preview',
        'model': 'gpt-4o',
        'max_tokens':max_tokens, 
        'temperature': temp,
        'top_p': 0.5,
        'messages':[]
    }
    if transcript is not None:
        data['messages'] = transcript

    response_text, retry, response_json = '', 0, None
    while len(response_text)<2:
        retry += 1
        try:
            response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, data=json.dumps(data)) 
            # response = requests.post(f"{BASE_URL}/chat/completions", headers=headers, data=json.dumps(data))
            response_json = response.json()
        except Exception as e:
            if random.random()<1: print(e)
            time.sleep(wait_time)
            continue
        if response.status_code != 200:
            print(response.headers,response.content)
            if random.random()<0.01: print(f"The response status code for is {response.status_code} (Not OK)")
            time.sleep(wait_time)
            data['temperature'] = min(data['temperature'] + 0.2, 1.0)
            continue
        if 'choices' not in response_json:
            time.sleep(wait_time)
            continue
        response_text = response_json["choices"][0]["message"]["content"]
    return response_json["choices"][0]["message"]["content"]


def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')



def load_img(image_path):
    base64_image = encode_image(image_path)
    image_meta = "data:image/png;base64" if 'png' in image_path else "data:image/jpeg;base64"
    img_dict = {
        "type": "image_url",
        "image_url": {
          "url": f"{image_meta},{base64_image}",
          "detail": "low"
        }
    }
    return img_dict


def GPT_process_single_image(image_path, api_key):

    client = OpenAI(api_key=api_key)

    prompt = '''
    Analyze the image generated by AI and identify any artifacts or inconsistencies present. Focus on areas such as unnatural edges, blending issues, unrealistic textures, or irregular object relationships. Make sure your answer is specific to the image and not too broad. Start your response with “However,” please.
    '''

    try:
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    }
                ]
            }
        ]

        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=256
        )

        chat_response = completion
        answer = chat_response.choices[0].message.content
        answer = answer.replace("However,", "").strip()
        return answer

    except FileNotFoundError:
        return f"Error: File '{image_path}' not found. Please check the path and try again."
    except Exception as e:
        return f"An error occurred: {e}"


class t2i_sd35():
    def __init__(self, args):
        model_id = args.diffusion_model_path
        self.pipe = StableDiffusion3Pipeline.from_pretrained(model_id, torch_dtype=torch.float16)
        self.pipe.to("cuda")

    def inference(self, prompt, savename):
        image = self.pipe(prompt).images[0]
        image.save(savename)

# class t2i_sdxl():
#     def __init__(self):
#         # from diffusers import DiffusionPipeline
#         model_id = ""
#         self.pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
#         self.pipe.to("cuda")
#     def inference(self, prompt, savename):
#         image = self.pipe(prompt).images[0]
#         image.save(savename)

# class t2i_sd2_1():
#     def __init__(self):
#         # from diffusers import DiffusionPipeline
#         model_id = ""
#         self.pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
#         self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)
#         self.pipe.to("cuda")
#     def inference(self, prompt, savename):
#         image = self.pipe(prompt).images[0]
#         image.save(savename)


def call_reflection_inference(args, image_path, conda_env="legion", output_dir="./output"):
    """
    Call the GLaMM inference function in a specified Conda environment.

    Args:
        model_path (str): Path to the HuggingFace model.
        image_path (str): Path to the input image.
        conda_env (str): Name of the Conda environment to run the inference.
        output_dir (str): Directory to save the output JSON file.

    Returns:
        dict: A dictionary containing the inference results (caption, phrases, and masks).
    """
    # Build the command to run glamm_inference.py in the specified Conda environment
    command = [
        "conda", "run", "-n", conda_env,
        "python", "reflection_infer.py",
        "--img_path", image_path,
        "--legion_model_path", args.legion_model_path,
        # "--output_dir", output_dir
    ]

    # Run the command
    result = subprocess.run(command, capture_output=True, text=True)

    # Check if the command ran successfully
    if result.returncode != 0:
        raise RuntimeError(f"inference failed with error:\n{result.stderr}")


    return result.stdout


def run_iterative_optimization(user_prompt, args, t2i_model, sub_folder, max_rounds=3):

    history = {
        "rounds": [],
        "images": [],
        "prompts": [],
        "reflections": [],
        "scores": []
    }

    current_prompt = user_prompt
    # history["prompts"].append(current_prompt)  

    for round in range(max_rounds):
        print(f"\n=== Round {round + 1} ===")


        if round > 0: 
            new_prompts = optimize_image_with_reflection(
                user_prompt=current_prompt,
                img_prompt=None,
                idea_transcript=["IDEA: " + current_prompt, "End of IDEA."],
                image_history=history["images"],
                prompt_history=history["prompts"],
                reflection_history=history["reflections"],
                args=args,
                reflection=reflection
            )
            current_prompt = new_prompts[0]  
            print(f"Optimized Prompt: {current_prompt}")

        image_path = f"{args.foldername}/{sub_folder}/round_{round + 1}_image.png"
        os.makedirs(os.path.dirname(image_path), exist_ok=True)

        # current_prompt = current_prompt + " Please generate realistic style image."
        t2i_model.inference(prompt=current_prompt, savename=image_path)
        history["images"].append(image_path)
        reflection = call_reflection_inference(args, image_path=image_path)

        history["reflections"].append(reflection)
        print(f"Reflection: {reflection}")

        history["rounds"].append(round + 1)
        history["prompts"].append(current_prompt)  
        score = hpsv2_score(image_path, current_prompt)
        print(f"Score: {score}")
        history["scores"].append(float(score))

    
    hps_v2_image_plot_path = f"{args.foldername}/{sub_folder}/hps_v2.png"
    plot_iteration_results(history, hps_v2_image_plot_path)

    return history

def plot_iteration_results(history, save_path):

    rounds = history["rounds"]
    scores = history["scores"]

    plt.figure(figsize=(10, 6))
    plt.plot(rounds, scores, marker='o', linestyle='-', color='b')
    plt.title("Iteration Results")
    plt.xlabel("Round")
    plt.ylabel("Score")
    plt.grid(True)
    plt.savefig(save_path)
    plt.show()



def optimize_image_with_reflection(user_prompt, img_prompt, idea_transcript, image_history, prompt_history, reflection_history, args, reflection):

    current_round = len(image_history)
    transcript = [{ "role": "system", "content": [] }, {"role": "user", "content": []}]
    
    # 系统提示
    transcript[0]["content"].append("You are a helpful assistant.\n\nInstruction: Given a user prompt, your task is to refine the prompt to improve generated image.\n")
    transcript[0]["content"].append("Here are some rules to write good prompts:\n")
    transcript[0]["content"].append("- Each prompt should consist of a description of the scene followed by modifiers divided by commas.\n")
    transcript[0]["content"].append("- The modifiers should alter the mood, style, lighting, spatial details, and other aspects of the scene.\n")
    transcript[0]["content"].append("- When generating prompts, reduce abstract psychological and emotional descriptions.\n")
    transcript[0]["content"].append("- Explain images and unusual entities in IDEA with detailed descriptions of the scene.\n")
    transcript[0]["content"].append("- Do not mention 'given image' in output; use detailed texts to describe the image in IDEA.\n")
    transcript[0]["content"].append("- Generate diverse prompts.\n")
    transcript[0]["content"].append("- Output prompt should have less than 50 words.\n")
    
    transcript[-1]["content"].append(f"user_prompt:{user_prompt}")
    transcript[-1]["content"].append("You are iteratively improving the user prompt by the given reflection.\n")
    transcript[-1]["content"].append("Reflection: %s\n" % reflection)

    transcript[-1]["content"].append("Based on the above information, to improve the image, you will write %d detailed prompts following the rules. Avoid generating prompts identical to the ones in previous rounds. Each prompt is wrapped with <START> and <END>.\n" % args.num_prompt)

    transcript[0]["content"] = "\n".join(transcript[0]["content"])  # 转换系统提示部分
    transcript[-1]["content"] = "\n".join(transcript[-1]["content"])  # 转换用户提示部分

    # print(f"\033[94mtranscript: {transcript}\033[0m")
    
    response = gptv_query(transcript)
    if '<START>' not in response or '<END>' not in response:  # one format retry
        response = gptv_query(transcript, temp=0.1)
    if args.verbose:
        print('gptv_revision_prompt    IDEA: %s.\n %s\n' % (user_prompt, response))
    
    prompts = response.split('<START>')[1:]
    prompts = [x.strip().split('<END>')[0] for x in prompts]
    while len(prompts) < args.num_prompt:
        prompts = prompts + ['blank image']
    return prompts


# Load the JSON file
def load_json_samples(json_file):
    with open(json_file, 'r') as f:
        samples = json.load(f)
    return samples

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--api_key", type=str, default="your_api_key", help="OpenAI GPT-4V API key")
    parser.add_argument("--num_prompt", type=int, default=1, help="number of prompts to search each round")
    parser.add_argument("--verbose", default=False, action="store_true")
    parser.add_argument("--json_file", type=str, default="", help="Path to the JSON file containing samples")
    parser.add_argument("--max_rounds", type=int, default=3, help="max number of iter rounds")
    parser.add_argument("--foldername", type=str, default="", help="folder name to save results")
    parser.add_argument("--case_num", type=int, default=3, help="case number")
    parser.add_argument("--legion_model_path", type=str, default="", help="legion model path")
    parser.add_argument("--diffusion_model_path", type=str, default="", help="diffusion model path")
    args = parser.parse_args()

    global api_key
    api_key = args.api_key

    # Load samples from JSON file
    samples = load_json_samples(args.json_file)

    # Initialize the model
    t2i_model = t2i_sd35(args)

    all_histories = {}  # To store histories for all samples

    # i = 0
    for image_path, user_prompt in samples.items():
        sub_folder = os.path.basename(image_path).split('.')[0]

        # if os.path.exists(f"{args.foldername}/{sub_folder}"):
        #     print(f"Sub-folder {sub_folder} already exists. Skipping...")
        #     i += 1
        #     print(f"skip {i} images.")
        #     continue 

        print(f"\n=== Processing Image: {image_path} ===")
        history = run_iterative_optimization(user_prompt, args, t2i_model, sub_folder, max_rounds=args.max_rounds)

        # Save history for each image
        save_history_path = f"{args.foldername}/{os.path.basename(image_path).split('.')[0]}/history.json"
        os.makedirs(os.path.dirname(save_history_path), exist_ok=True)
        with open(save_history_path, "w") as f:
            json.dump(history, f, indent=4, ensure_ascii=False)

        all_histories[image_path] = history  # Store the history for this image

    # Optionally, save all histories in one file
    with open(f"{args.foldername}/all_histories.json", "w") as f:
        json.dump(all_histories, f, indent=4, ensure_ascii=False)
